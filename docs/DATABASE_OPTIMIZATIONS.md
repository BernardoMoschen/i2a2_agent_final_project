# Otimiza√ß√µes de Banco de Dados - SQLite Performance

## üìä An√°lise Atual e Oportunidades de Otimiza√ß√£o

### Estado Atual

- **Banco**: SQLite (bom para desenvolvimento e small-medium workloads)
- **Tabelas**: 4 (invoices, invoice_items, validation_issues, classification_cache)
- **√çndices**: 15 √≠ndices criados automaticamente pelos Fields
- **Relacionamentos**: Cascata de dele√ß√£o configurada
- **Transa√ß√µes**: N√£o otimizadas para batch insert

---

## üöÄ Otimiza√ß√µes Implement√°veis

### 1Ô∏è‚É£ **SQLite Performance Pragmas** (CR√çTICO)

**Problema**: SQLite usa configura√ß√µes conservadoras por default.

**Solu√ß√£o**: Adicionar PRAGMAs na conex√£o para otimizar performance.

```python
# Em DatabaseManager.__init__()
from sqlalchemy import event

@event.listens_for(self.engine, "connect")
def set_sqlite_pragma(dbapi_conn, connection_record):
    cursor = dbapi_conn.cursor()
    # Journal mode WAL permite leituras concorrentes
    cursor.execute("PRAGMA journal_mode=WAL")
    # Sincroniza√ß√£o otimizada (NORMAL √© seguro e mais r√°pido)
    cursor.execute("PRAGMA synchronous=NORMAL")
    # Cache de p√°ginas maior (10MB)
    cursor.execute("PRAGMA cache_size=-10000")
    # Habilita chaves estrangeiras
    cursor.execute("PRAGMA foreign_keys=ON")
    # Temp store em mem√≥ria
    cursor.execute("PRAGMA temp_store=MEMORY")
    # Mmap para leituras mais r√°pidas (256MB)
    cursor.execute("PRAGMA mmap_size=268435456")
    cursor.close()
```

**Impacto**:

- ‚úÖ **2-5x mais r√°pido** em opera√ß√µes de escrita
- ‚úÖ **Leituras concorrentes** sem bloqueio
- ‚úÖ **Menor lat√™ncia** em queries

---

### 2Ô∏è‚É£ **Bulk Insert API** (ALTO IMPACTO)

**Problema**: `save_invoice()` faz commit individual (lento para batches).

**Solu√ß√£o**: Criar m√©todo para batch insert.

```python
def save_invoices_batch(
    self,
    invoices_data: List[Tuple[InvoiceModel, List, Optional[dict]]]
) -> List[InvoiceDB]:
    """
    Save multiple invoices in a single transaction.

    Args:
        invoices_data: List of (invoice_model, validation_issues, classification)

    Returns:
        List of saved InvoiceDB instances
    """
    saved = []

    with Session(self.engine) as session:
        for invoice_model, validation_issues, classification in invoices_data:
            # Check duplicates
            existing = session.exec(
                select(InvoiceDB).where(
                    InvoiceDB.document_key == invoice_model.document_key
                )
            ).first()

            if existing:
                logger.warning(f"Skipping duplicate: {invoice_model.document_key}")
                saved.append(existing)
                continue

            # Create invoice (sem commit individual)
            invoice_db = self._create_invoice_db(invoice_model, classification)
            session.add(invoice_db)
            session.flush()  # Get ID without committing

            # Add items and issues
            for item in invoice_model.items:
                item_db = self._create_item_db(invoice_db.id, item)
                session.add(item_db)

            for issue in validation_issues:
                issue_db = self._create_issue_db(invoice_db.id, issue)
                session.add(issue_db)

            saved.append(invoice_db)

        # Single commit for all
        session.commit()

        # Refresh all at once
        for inv in saved:
            session.refresh(inv)

    return saved
```

**Impacto**:

- ‚úÖ **10-50x mais r√°pido** para importar ZIP com m√∫ltiplos XMLs
- ‚úÖ Reduz I/O disk (um √∫nico commit)
- ‚úÖ Transa√ß√£o at√¥mica (all-or-nothing)

---

### 3Ô∏è‚É£ **√çndices Compostos** (M√âDIO IMPACTO)

**Problema**: Queries com m√∫ltiplos filtros fazem full scan.

**Solu√ß√£o**: Criar √≠ndices compostos para queries comuns.

```python
from sqlalchemy import Index

class InvoiceDB(SQLModel, table=True):
    # ... campos existentes ...

    # √çndices compostos para queries frequentes
    __table_args__ = (
        # Busca por per√≠odo + tipo
        Index('ix_invoices_date_type', 'issue_date', 'document_type'),

        # Busca por emitente + per√≠odo
        Index('ix_invoices_issuer_date', 'issuer_cnpj', 'issue_date'),

        # Busca por destinat√°rio + per√≠odo
        Index('ix_invoices_recipient_date', 'recipient_cnpj_cpf', 'issue_date'),

        # Busca por centro de custo + tipo opera√ß√£o
        Index('ix_invoices_cost_center_op', 'cost_center', 'operation_type'),

        # Busca transport: modal + per√≠odo
        Index('ix_invoices_modal_date', 'modal', 'issue_date'),

        # Busca transport: RNTRC + tipo
        Index('ix_invoices_rntrc_type', 'rntrc', 'document_type'),
    )
```

**Impacto**:

- ‚úÖ **5-20x mais r√°pido** em queries com filtros compostos
- ‚úÖ Dashboards e relat√≥rios mais responsivos

---

### 4Ô∏è‚É£ **Otimiza√ß√£o de Relacionamentos** (BAIXO IMPACTO)

**Problema**: N+1 queries ao carregar invoice + items + issues.

**Solu√ß√£o**: J√° est√° usando `selectinload` corretamente! ‚úÖ

**Verifica√ß√£o adicional**: Garantir uso consistente.

```python
def get_invoice_by_key(self, document_key: str) -> Optional[InvoiceDB]:
    """Get invoice with eager loading of relationships."""
    with Session(self.engine) as session:
        statement = (
            select(InvoiceDB)
            .where(InvoiceDB.document_key == document_key)
            .options(
                selectinload(InvoiceDB.items),
                selectinload(InvoiceDB.issues)
            )
        )
        return session.exec(statement).first()
```

---

### 5Ô∏è‚É£ **Compress√£o do raw_xml** (M√âDIO IMPACTO)

**Problema**: Campo `raw_xml` ocupa muito espa√ßo (10-50KB por doc).

**Solu√ß√£o**: Comprimir XML antes de salvar.

```python
import gzip
import base64

class InvoiceDB(SQLModel, table=True):
    # Trocar raw_xml por compressed_xml
    compressed_xml: Optional[str] = Field(default=None)

    def set_raw_xml(self, xml_str: str):
        """Compress and store XML."""
        if xml_str:
            compressed = gzip.compress(xml_str.encode('utf-8'))
            self.compressed_xml = base64.b64encode(compressed).decode('ascii')

    def get_raw_xml(self) -> Optional[str]:
        """Decompress and return XML."""
        if self.compressed_xml:
            compressed = base64.b64decode(self.compressed_xml.encode('ascii'))
            return gzip.decompress(compressed).decode('utf-8')
        return None
```

**Impacto**:

- ‚úÖ **70-80% redu√ß√£o** no tamanho do banco
- ‚úÖ Backups mais r√°pidos
- ‚ö†Ô∏è Pequeno overhead em CPU (aceit√°vel)

---

### 6Ô∏è‚É£ **Particionamento de raw_xml** (OPCIONAL)

**Problema**: raw_xml aumenta tamanho da tabela principal.

**Solu√ß√£o**: Mover para tabela separada.

```python
class InvoiceXMLDB(SQLModel, table=True):
    """Separate table for XML storage."""

    __tablename__ = "invoice_xmls"

    invoice_id: int = Field(foreign_key="invoices.id", primary_key=True)
    compressed_xml: str
    created_at: datetime = Field(default_factory=lambda: datetime.now(UTC))

# Em InvoiceDB, adicionar relationship:
class InvoiceDB(SQLModel, table=True):
    xml_storage: Optional["InvoiceXMLDB"] = Relationship()
```

**Impacto**:

- ‚úÖ Queries em `invoices` s√£o mais r√°pidas (tabela menor)
- ‚úÖ XML s√≥ √© carregado quando necess√°rio
- ‚ö†Ô∏è Complexidade adicional

---

### 7Ô∏è‚É£ **Cache de Queries Frequentes** (BAIXO IMPACTO)

**Problema**: Mesmas queries executadas repetidamente.

**Solu√ß√£o**: Usar cache em mem√≥ria (functools.lru_cache).

```python
from functools import lru_cache
from datetime import datetime, timedelta

class DatabaseManager:

    @lru_cache(maxsize=100)
    def get_invoice_count_by_type(self, document_type: str) -> int:
        """Cached count query."""
        with Session(self.engine) as session:
            statement = select(InvoiceDB).where(
                InvoiceDB.document_type == document_type
            )
            return len(session.exec(statement).all())

    def clear_cache(self):
        """Clear all cached queries."""
        self.get_invoice_count_by_type.cache_clear()
```

**Impacto**:

- ‚úÖ **Instant√¢neo** para queries repetidas
- ‚ö†Ô∏è Requer invalida√ß√£o manual ao inserir/deletar

---

### 8Ô∏è‚É£ **Vacuum e Analyze Autom√°tico** (MANUTEN√á√ÉO)

**Problema**: SQLite acumula espa√ßo n√£o usado.

**Solu√ß√£o**: Agendar VACUUM e ANALYZE.

```python
def optimize_database(self):
    """Run VACUUM and ANALYZE to optimize database."""
    with self.engine.begin() as conn:
        logger.info("Running VACUUM...")
        conn.exec_driver_sql("VACUUM")
        logger.info("Running ANALYZE...")
        conn.exec_driver_sql("ANALYZE")
        logger.info("Database optimization complete")
```

**Uso**: Executar mensalmente ou ap√≥s bulk deletes.

---

## üìà Prioriza√ß√£o de Implementa√ß√£o

### ‚ö° **CR√çTICAS (Implementar Imediatamente)**

1. ‚úÖ **SQLite PRAGMAs** - 5 minutos, 2-5x speedup
2. ‚úÖ **Bulk Insert API** - 30 minutos, 10-50x speedup em batches

### üéØ **ALTAS (Implementar Esta Semana)**

3. ‚úÖ **√çndices Compostos** - 15 minutos, 5-20x speedup em queries
4. ‚úÖ **Compress√£o XML** - 45 minutos, 70% redu√ß√£o de espa√ßo

### üìä **M√âDIAS (Implementar Quando Necess√°rio)**

5. ‚è≥ **Particionamento XML** - 1-2 horas, queries mais r√°pidas
6. ‚è≥ **VACUUM Autom√°tico** - 30 minutos, manuten√ß√£o preventiva

### üîÆ **BAIXAS (Nice to Have)**

7. ‚è≥ **Query Cache** - 30 minutos, benef√≠cio limitado com SQLite
8. ‚è≥ **Migra√ß√£o para PostgreSQL** - 4-8 horas, quando escalar muito

---

## üß™ Benchmarks Esperados

### Antes das Otimiza√ß√µes

- **Insert single**: ~50ms
- **Insert batch (100 docs)**: ~5000ms (50ms cada)
- **Query com filtros**: ~100-500ms
- **Tamanho DB (1000 docs)**: ~50MB

### Depois das Otimiza√ß√µes

- **Insert single**: ~20ms (PRAGMAs)
- **Insert batch (100 docs)**: ~200ms (bulk insert)
- **Query com filtros**: ~10-50ms (√≠ndices compostos)
- **Tamanho DB (1000 docs)**: ~15MB (compress√£o)

**Ganho Total**: **10-25x mais r√°pido** + **70% menos espa√ßo**

---

## üîß Implementa√ß√£o Passo a Passo

### Passo 1: PRAGMAs (5 min)

```python
# Adicionar em DatabaseManager.__init__()
from sqlalchemy import event

@event.listens_for(self.engine, "connect")
def set_sqlite_pragma(dbapi_conn, connection_record):
    cursor = dbapi_conn.cursor()
    cursor.execute("PRAGMA journal_mode=WAL")
    cursor.execute("PRAGMA synchronous=NORMAL")
    cursor.execute("PRAGMA cache_size=-10000")
    cursor.execute("PRAGMA foreign_keys=ON")
    cursor.execute("PRAGMA temp_store=MEMORY")
    cursor.execute("PRAGMA mmap_size=268435456")
    cursor.close()
```

### Passo 2: Bulk Insert (30 min)

- Criar m√©todo `save_invoices_batch()`
- Testar com ZIP de 100 XMLs
- Comparar tempo antes/depois

### Passo 3: √çndices Compostos (15 min)

- Adicionar `__table_args__` em InvoiceDB
- Executar migra√ß√£o ou recriar banco
- Testar queries de dashboard

### Passo 4: Compress√£o XML (45 min)

- Adicionar campo `compressed_xml`
- Criar m√©todos `set_raw_xml()` e `get_raw_xml()`
- Migrar dados existentes (script)
- Atualizar parsers e validators

---

## ‚úÖ Checklist de Implementa√ß√£o

- [ ] Implementar SQLite PRAGMAs
- [ ] Criar bulk insert API
- [ ] Adicionar √≠ndices compostos
- [ ] Implementar compress√£o de XML
- [ ] Adicionar m√©todo VACUUM autom√°tico
- [ ] Criar testes de performance
- [ ] Documentar uso de bulk insert
- [ ] Atualizar FileProcessor para usar bulk

---

## üìö Recursos Adicionais

- [SQLite Performance Tips](https://www.sqlite.org/performance.html)
- [SQLAlchemy Performance](https://docs.sqlalchemy.org/en/14/faq/performance.html)
- [WAL Mode](https://www.sqlite.org/wal.html)

---

**Data**: 28 de outubro de 2025  
**Autor**: AI Agent (GitHub Copilot)
